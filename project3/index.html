<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Project 3 — Object Detection and Human–Object Interaction</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="./style.css">
</head>
<body>
  <header class="site-header">
    <div class="header-inner">
      <div class="brand">
        <a class="back" href="../">&larr; Home</a>
        <h1>Project 3 — Object Detection & Human–Object Interaction</h1>
        <p class="byline">Compton Ross</p>
      </div>
      <nav class="toc">
        <a href="#part1">Part 1</a>
        <a href="#part2">Part 2</a>
      </nav>
    </div>
  </header>

  <main>
    <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2>Part 1 — Lightweight Object Detection (YOLO on Bananas)</h2>
      <p class="summary">
        In Part&nbsp;1 I implemented a YOLO object detector from scratch and train it on the
        banana detection dataset from d2l. The model uses an 8&times;8 prediction grid over a
        256&times;256 image and outputs a bounding box, objectness, and class score per cell.
      </p>
    </section>

    <!-- ===== Part 1 — Training Curve ===== -->
    <section id="part1-training">
      <h3>Training Curve</h3>
      <p class="summary">
        The detector is trained with a loss specified for YOLO. A MSE loss on normalized box coordinates for positive cells,
        a binary cross-entropy loss on objectness for both positive and negative cells (negatives are down weighted),
        and a class loss for positive cells. The coordinate loss encourages localization, the objectness loss penalizes false positives, and the class loss helps to ensure
        that only cells containing the object will classify it correctly. I trained the model using SGD with momentum.
      </p>
      <div class="grid one">
        <figure class="card">
          <img src="./media/part_a/image_1.png" alt="Training loss curve for YOLO banana detector">
          <figcaption>Training loss across epochs (total, coordinate, and objectness components).</figcaption>
        </figure>
      </div>
      <p class="discussion">
        Over training, the total loss decreases and appears to converge at some value. Coordinate loss appears to drop slightly more slowly than the
        objectness loss, which is expected because localization is harder than deciding
        whether a banana is present. It also appears that the largeset decrease in loss happens over the first few epochs, which then seem to slowly converge in the following ones.
        By the end of training the model consistently produces high scores on grid cells
        near the banana and low scores elsewhere (in the validation set).
      </p>
    </section>

    <!-- ===== Part 1 — Sample Detections on Validation Set ===== -->
    <section id="part1-val">
      <h3>Sample Banana Detections</h3>
      <p class="summary">
        5 sample detections from the validation set, each with their bounding boxes, are shown below.
      </p>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part_a/image_2.png" alt="Validation detection 1">
          <figcaption>Sample Detection 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_3.png" alt="Validation detection 2">
          <figcaption>Sample Detection 2</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_4.png" alt="Validation detection 3">
          <figcaption>Sample Detection 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_5.png" alt="Validation detection 4">
          <figcaption>Sample Detection 4</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_6.png" alt="Validation detection 5">
          <figcaption>Sample Detection 5</figcaption>
        </figure>
      </div>
      <p class="discussion">
        On most of the images, the highest scoring prediction lies over the banana and yields a decent
        bounding box. Also, there are several cases where a cluster of nearby boxes with similar scores is created,
        which is will be taken care of when NMS is implemented in part 2.
      </p>
    </section>

    <!-- ===== Part 1 — Custom Images & Failure Cases ===== -->
    <section id="part1-custom">
      <h3>Testing on My Own Images</h3>
      <p class="summary">
        I took a variety of banana photos to test the model and see if it could applied to real-world, realistic images. Additionally, two copyright free stock images of bananas are
        also used for testing and benchmark purposes
      </p>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part_a/image_7.png" alt="Custom banana image 1">
          <figcaption>Custom Image 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_8.png" alt="Custom banana image 2">
          <figcaption>Custom Image 2</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_9.png" alt="Custom banana image 3">
          <figcaption>Custom Image 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_10.png" alt="Custom banana image 4">
          <figcaption>Custom Image 4</figcaption>
        </figure>
      </div>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part_a/image_11.png" alt="Custom banana image 5">
          <figcaption>Custom Image 5</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_12.png" alt="Custom banana image 6">
          <figcaption>Custom Image 6</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_13.png" alt="Custom banana image 7">
          <figcaption>Online Image 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/image_14.png" alt="Custom banana image 8">
          <figcaption>Online Image 2</figcaption>
        </figure>
      </div>
      <p class="discussion">
        The model appears to work somewhat well in a few cases, but not all of them. When testing, I opted to take pictures using an older (yellow/brown) and newer (green/yellow) banana.
        I included images of bananas on tables, in reflections, as a background object, and as a foreground object (with noisy background). In most cases, it appears that some part of the banana 
        was picked up in a few or several bounding boxes by the model. The two biggest issues the model seemed to struggle with most was noisy backgrounds (As seen in the top of image 1, and in image 6),
        as well as when the banana is not the main focus (it was not picked up at all in the background of image 5). It can be noted, though, the probabilities (objecness score * class score) of these bananas
        are extremely low, especially when compared to that of the validation images. Sample plain pictures of bananas were used for testing as well, and followed the same issue (although they appear much more similar to the training data).
        A potential cause for this would be from resizing my own images. The original images frmo the d2l dataset were originally created as 256 x 256. The photos took from my phone were at a significantly higher resolution,
        but were then resized for detection purposes. Due to the drastic drop in resolution, several details including aspect ratio and the detail of the banana could be lost. Although it can detect some sort of 
        signifance at or around the banana in most images, the scores they receive are extremely low.
      </p>
    </section>

    <!-- ===================== Part 2 ===================== -->
    <section id="part2">
      <h2>Part 2 — Non-Maximum Suppression (NMS)</h2>
      <p class="summary">
        I implemented a version of Non Maximum Suppression (NMS) and apply it to the YOLO outputs
        from Part&nbsp;1. NMS removes redundant overlapping detections by keeping only the highest-scoring box when
        multiple boxes overlap with high IoU. My implementation is then compared to torchvision's.
      </p>
    </section>

    <!-- ===== Part 2 — Before / After NMS Comparisons ===== -->
    <section id="part2-nms-vis">
      <h3>YOLO Outputs Before and After NMS</h3>
      <p class="summary">
        For each sample image from part 1, the YOLO grid predictions, including teh bounding boxes and confidence scores are shown.
        The comparison figures show the high-scoring boxes before NMS, the boxes kept by my
        implementation, and the boxes kept by PyTorch's NMS. In both my implementation and pytorch's version the same boxes are kept.
      </p>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/image_1_comparison.png" alt="NMS comparison 1">
          <figcaption>Comparison 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/image_2_comparison.png" alt="NMS comparison 2">
          <figcaption>Comparison 2</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/image_3_comparison.png" alt="NMS comparison 3">
          <figcaption>Comparison 3</figcaption>
        </figure>
      </div>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/image_4_comparison.png" alt="NMS comparison 4">
          <figcaption>Comparison 4</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/image_5_comparison.png" alt="NMS comparison 5">
          <figcaption>Comparison 5</figcaption>
        </figure>
      </div>
      <p class="discussion">
        In most of the examples, NMS reduces the number of detections and leaves a small set of boxes. 
        For my implementation, I used a greedy algorithm where I sorted boxes by score, kept the
        highest scoring box(es), and discard any remaining boxes whose IoU did not meet a certain threshold. 
        This process was repeated until no boxes remain. The most notable reductions in boxes were on images 3 and 5, with no improvements made in image 2.
        The output of my implementation matched the output of torchvision's in all samples.
      </p>
    </section>

    <!-- ===== Part 2 — Discussion ===== -->
    <section id="part2-discussion">
      <h3>Purpose and Limitations</h3>
      <p class="discussion">
        NMS is essential for detectors that output dense, overlapping predictions. Several overlapping predictions can make final results hard to interpret, and could possibly inflate detection counts. 
        However, NMS is not perfect. Using a single IoU threshold can incorrectly suppress nearby
        instances of the same class (for example, if two bananas overlap slightly), and running NMS independently
        per class ignores relationships between categories. NMS also cannot fix localization errors. If the
        object detector constantly predicts boxes that are slightly off center of the real object, NMS will still keep those off center boxes.
      </p>
    </section>

  </main>

  <footer class="site-footer">
    <p>© <span id="year"></span> Compton Ross</p>
  </footer>

  <!-- Zoom modal -->
  <div id="imgModal" class="modal">
    <span id="modalClose" class="close">&times;</span>
    <img class="modal-content" id="modalImg" alt="">
    <div id="caption"></div>
  </div>

  <script>
    // Click-to-zoom behavior
    const modal = document.getElementById("imgModal");
    const modalImg = document.getElementById("modalImg");
    const captionText = document.getElementById("caption");
    const closeBtn = document.getElementById("modalClose");

    document.querySelectorAll("section img").forEach(img => {
      img.style.cursor = "zoom-in";
      img.addEventListener("click", () => {
        modal.style.display = "block";
        modalImg.src = img.src;
        captionText.textContent =
          img.nextElementSibling ? img.nextElementSibling.innerText : "";
      });
    });

    closeBtn.addEventListener("click", () => {
      modal.style.display = "none";
    });

    modal.addEventListener("click", (e) => {
      if (e.target === modal) modal.style.display = "none";
    });

    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
