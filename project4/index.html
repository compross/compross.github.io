<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Project 4 — Diffusion Models</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="./style.css">
</head>
<body>
  <header class="site-header">
    <div class="header-inner">
      <div class="brand">
        <a class="back" href="../">&larr; Home</a>
        <h1>Project 4 — Diffusion Models</h1>
        <p class="byline">Compton Ross</p>
      </div>
      <nav class="toc">
        <a href="#part0">Part 0</a>
        <a href="#part1">Part 1</a>
        <a href="#part2">Part 2</a>
        <a href="#part3">Part 3</a>

      </nav>
    </div>
  </header>

  <main>
        <!-- ===================== Part 0 ===================== -->
    <section id="part0">
      <h2>Part 0 — Setup & Effect of Sampling Steps</h2>
      <p class="summary">
        In Part&nbsp;0 I set up the DeepFloyd IF pipeline on Hugging Face, loaded the provided
        prompt embeddings, and generated images for the three given prompts:
        <em>"a rocket ship"</em>,
        <em>"an oil painting of a snowy mountain village"</em>, and
        <em>"a man wearing a hat"</em>.
        For each prompt I compare samples generated with a small number of denoising steps
        (<code>num_inference_steps = 5</code>) and a larger number of steps
        (<code>num_inference_steps = 20</code>) using the same random seed.
        This highlights the speed–quality trade-off in diffusion sampling.
      </p>

      <!-- ===== Rocket Ship Samples ===== -->
      <h3>Rocket Ship — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/rocket_5.png" alt="Rocket ship with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The overall rocket shape is visible, but the scene is very noisy
            and grainy. Background structure is muddled and fine details are poorly defined.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/rocket_20.png" alt="Rocket ship with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> The rocket is crisp with clean edges, smooth gradients, and
            clear light sources. Increasing the number of steps greatly improves coherence and detail.
          </figcaption>
        </figure>
      </div>

      <!-- ===== Snowy Village Samples ===== -->
      <h3>Snowy Mountain Village — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/oil_painting_5.png" alt="Snowy village with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The village is only loosely defined; buildings and trees blur
            into a textured pointillist pattern. The scene feels abstract and lacks clear depth.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/oil_painting_20.png" alt="Snowy village with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> Houses, trees, and snow banks are sharp and stylized, with
            strong color contrasts and readable geometry. Longer sampling recovers a much more
            coherent landscape.
          </figcaption>
        </figure>
      </div>

      <!-- ===== Man Wearing a Hat Samples ===== -->
      <h3>Man Wearing a Hat — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/man_5.png" alt="Man in hat with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The portrait is heavily textured and stylized, with facial
            features still recognizable but soft and noisy. The image looks more like a rough
            pointillist painting.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/man_20.png" alt="Man in hat with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> The face, hat, and clothing are sharply defined and realistic.
            Noise is largely removed, and lighting and shading look much more natural.
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        Across all three prompts, using only <code>5</code> denoising steps produces quick but noisy
        outputs with blurred edges and unstable structure. Increasing to
        <code>20</code> steps substantially improves global coherence and fine detail, but at the
        cost of additional compute time. For the remainder of the project I use a moderate number of
        sampling steps to balance runtime with visual quality.
      </p>
    </section>


        <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2>Part 1 — Forward &amp; Reverse Processes</h2>
      <p class="summary">
        Part&nbsp;1 focuses on how diffusion models add and remove noise. I first implement the
        forward noising process, then compare traditional Gaussian denoising to a learned UNet,
        and finally run an iterative DDPM-style reverse process starting from a highly corrupted
        image.
      </p>
    </section>

    <!-- ===== Part 1.1 — Forward Process ===== -->
    <section id="part1-forward">
      <h3>1.1 Forward Process (Adding Noise)</h3>
      <p class="summary">
        The forward process corrupts a clean image <code>x₀</code> into a noisy version
        <code>x<sub>t</sub></code> according to
        <code>x<sub>t</sub> = √&alpha;̄<sub>t</sub>x₀ + √(1−&alpha;̄<sub>t</sub>)&epsilon;</code>.
        Below I show the original image and its noised versions at
        <code>t = 250, 500, 750</code> using the provided noise schedule
        <code>alphas_cumprod</code>.
      </p>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original test image">
          <figcaption>Original test image <code>x₀</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code>; content is still very recognizable.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption><code>t = 500</code>; edges soften and textures start to disappear.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption><code>t = 750</code>; the image is close to pure Gaussian noise.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        As <code>t</code> increases, the signal is both rescaled and mixed with noise. At medium
        timesteps the global structure is barely visible, and by <code>t = 750</code> almost all
        semantic information has been destroyed, which motivates the need for a strong reverse
        process.
      </p>
    </section>

    <!-- ===== Part 1.2 — Gaussian Denoising ===== -->
    <section id="part1-gaussian">
      <h3>1.2 Traditional Denoising (Gaussian Blur)</h3>
      <p class="summary">
        I attempt to recover the original image from the noisy versions using Gaussian blur
        (a purely classical, non-learned method). For each timestep I show the noisy input and
        the best Gaussian-denoised result I could obtain by tuning the blur parameters.
      </p>

      <h4>t = 250</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_250.png" alt="Gaussian denoised t=250">
          <figcaption>
            Gaussian denoised at <code>t = 250</code>; some noise is removed, but edges are noticeably blurred.
          </figcaption>
        </figure>
      </div>

      <h4>t = 500</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption>Noisy image at <code>t = 500</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_500.png" alt="Gaussian denoised t=500">
          <figcaption>
            Gaussian denoised at <code>t = 500</code>; the image becomes smoother but also loses most fine detail.
          </figcaption>
        </figure>
      </div>

      <h4>t = 750</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption>Noisy image at <code>t = 750</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_750.png" alt="Gaussian denoised t=750">
          <figcaption>
            Gaussian denoised at <code>t = 750</code>; almost all structure is washed out into a smooth texture.
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        Gaussian blur can reduce high-frequency noise, but it has no knowledge of natural image
        statistics. At low noise levels it produces passable results, but at higher timesteps it
        simply smooths everything, destroying edges and semantics. This illustrates why diffusion
        models rely on a learned denoiser instead of traditional filtering.
      </p>
    </section>

    <!-- ===== Part 1.3 — One-Step UNet Denoising ===== -->
    <section id="part1-unet">
      <h3>1.3 One-Step Denoising with a Pretrained UNet</h3>
      <p class="summary">
        I next use the pretrained Stage&nbsp;1 UNet (<code>stage_1.unet</code>) to predict the noise
        in <code>x<sub>t</sub></code> and reconstruct an estimate of the clean image via
        <code>ĥx₀ = (x<sub>t</sub> − √(1−&alpha;̄<sub>t</sub>) ĥ&epsilon;)/√&alpha;̄<sub>t</sub></code>.
        For each timestep I show the original, the noisy input, and the one-step denoised result.
      </p>

      <h4>t = 250</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=250">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_250.png" alt="One-step denoised t=250">
          <figcaption>One-step UNet denoised result at <code>t = 250</code>.</figcaption>
        </figure>
      </div>

      <h4>t = 500</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=500">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption>Noisy image at <code>t = 500</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_500.png" alt="One-step denoised t=500">
          <figcaption>One-step UNet denoised result at <code>t = 500</code>.</figcaption>
        </figure>
      </div>

      <h4>t = 750</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=750">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption>Noisy image at <code>t = 750</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_750.png" alt="One-step denoised t=750">
          <figcaption>One-step UNet denoised result at <code>t = 750</code>.</figcaption>
        </figure>
      </div>

      <p class="discussion">
        At moderate noise levels (e.g., <code>t = 250</code>), a single UNet pass produces a
        surprisingly accurate reconstruction. As the noise level increases, the one-step estimate
        still produces a plausible natural image but drifts further away from the exact original,
        showing that multiple steps are preferable when starting from heavily corrupted inputs.
      </p>
    </section>

    <!-- ===== Part 1.4 — Iterative Denoising ===== -->
    <section id="part1-iter">
      <h3>1.4 Iterative Denoising with Strided Timesteps</h3>
      <p class="summary">
        Finally, I implement an iterative reverse diffusion process using a strided schedule of
        timesteps. Starting from a very noisy image at <code>t = 690</code>, the model repeatedly
        predicts noise and steps toward <code>t = 0</code>, applying the DDPM update rule with
        learned variance. Below are snapshots of the image every few iterations.
      </p>

      <div class="grid six">
        <figure class="card">
          <img src="./media/part_b/iter_0.png" alt="Iterative denoise step 0">
          <figcaption>Iteration 0 (starting from <code>t = 690</code>): extremely noisy.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_5.png" alt="Iterative denoise step 5">
          <figcaption>Iteration 5: coarse shapes begin to emerge.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_10.png" alt="Iterative denoise step 10">
          <figcaption>Iteration 10: major structures are visible.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_15.png" alt="Iterative denoise step 15">
          <figcaption>Iteration 15: edges and textures sharpen.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_20.png" alt="Iterative denoise step 20">
          <figcaption>Iteration 20: image is close to a clean sample.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_22.png" alt="Iterative denoise final">
          <figcaption>Iteration 22: final iterative denoised image.</figcaption>
        </figure>
      </div>

      <h4>Final Comparison at t = 690</h4>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image">
          <figcaption>Original clean image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_690.png" alt="Noisy image t=690">
          <figcaption>Noisy image at <code>t = 690</code> (starting point).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iterative_690.png" alt="Iterative denoise result">
          <figcaption>Iterative denoising result (multiple DDPM steps).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_690.png" alt="One-step denoise result">
          <figcaption>One-step UNet denoise from <code>t = 690</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_690.png" alt="Gaussian blur result">
          <figcaption>Gaussian blur baseline from <code>t = 690</code>.</figcaption>
        </figure>
      </div>

      <p class="discussion">
        The sequence of iterated images shows the diffusion model gradually pulling the sample back
        onto the natural image manifold. Compared to the one-step UNet and Gaussian blur, iterative
        denoising produces the cleanest and most coherent result, especially when starting from a
        very noisy state. The Gaussian version is overly smooth and loses structure, while the
        one-step UNet tends to under-correct large amounts of noise.
      </p>
    </section>


        <!-- ===================== Part 2 ===================== -->
    <section id="part2">
      <h2>Part 2 — Sampling, Guidance, and Image-to-Image Translation</h2>
      <p class="summary">
        In Part&nbsp;2 I use the diffusion model purely generatively. First I sample images from
        random noise, then I apply classifier-free guidance (CFG) to boost image quality, and
        finally I perform SDEdit-style image-to-image translation on both a provided test image and
        two of my own photos.
      </p>
    </section>

    <!-- ===== Part 2.1 — Diffusion Model Sampling ===== -->
    <section id="part2-sampling">
      <h3>2.1 Diffusion Model Sampling from Noise</h3>
      <p class="summary">
        Using the <code>iterative_denoise</code> routine from Part&nbsp;1, I start from pure
        Gaussian noise and denoise all the way to <code>t = 0</code> with the prompt
        <em>"a high quality photo"</em>. The model gradually transforms random noise into coherent
        natural images.
      </p>

      <div class="grid five">
        <figure class="card">
          <img src="./media/part_c/2.1/sample_1.png" alt="Sampled image 1">
          <figcaption>Sample 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.1/sample_2.png" alt="Sampled image 2">
          <figcaption>Sample 2</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.1/sample_3.png" alt="Sampled image 3">
          <figcaption>Sample 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.1/sample_4.png" alt="Sampled image 4">
          <figcaption>Sample 4</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.1/sample_5.png" alt="Sampled image 5">
          <figcaption>Sample 5</figcaption>
        </figure>
      </div>

      <p class="discussion">
        Even without explicit class labels, the model generates diverse, realistic-looking scenes
        from pure noise. Some samples are more coherent than others, but overall the textures,
        lighting, and global structure reflect that the UNet has learned a powerful prior over
        natural images.
      </p>
    </section>

    <!-- ===== Part 2.2 — Classifier-Free Guidance (CFG) ===== -->
    <section id="part2-cfg">
      <h3>2.2 Classifier-Free Guidance (CFG)</h3>
      <p class="summary">
        Next I implement <code>iterative_denoise_cfg</code>, which combines conditional and
        unconditional noise predictions using classifier-free guidance. I use the same
        <em>"a high quality photo"</em> prompt but now apply a guidance scale of
        <code>&gamma; = 7</code> to sharpen the samples.
      </p>

      <div class="grid five">
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_1.png" alt="CFG sample 1">
          <figcaption>CFG Sample 1 (γ = 7)</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_2.png" alt="CFG sample 2">
          <figcaption>CFG Sample 2 (γ = 7)</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_3.png" alt="CFG sample 3">
          <figcaption>CFG Sample 3 (γ = 7)</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_4.png" alt="CFG sample 4">
          <figcaption>CFG Sample 4 (γ = 7)</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_5.png" alt="CFG sample 5">
          <figcaption>CFG Sample 5 (γ = 7)</figcaption>
        </figure>
      </div>

      <p class="discussion">
        Compared to the unguided samples, CFG images are noticeably sharper and more consistent:
        edges are cleaner, objects look more “intentional,” and backgrounds contain fewer random
        artifacts. The trade-off is reduced diversity—different runs tend to converge to similar
        styles and compositions as the model is pushed harder toward high-likelihood regions.
      </p>
    </section>

    <!-- ===== Part 2.3 — Image-to-Image Translation (SDEdit) ===== -->
    <section id="part2-sdedit">
      <h3>2.3 Image-to-Image Translation (SDEdit)</h3>
      <p class="summary">
        Finally, I use the SDEdit-style procedure: start from a real image, add noise to a chosen
        timestep, and then run CFG denoising back to <code>t = 0</code>. With low noise, the output
        stays close to the original; with high noise, the model “reimagines” the scene while
        preserving some coarse structure.
      </p>

      <!-- Provided test image edits -->
      <h4>Edits of the Provided Test Image</h4>
      <div class="grid seven">
        <figure class="card">
          <img src="./media/part_c/2.3/original.png" alt="Original test image for SDEdit">
          <figcaption>Original test image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_1.png" alt="Edit starting at 1 step">
          <figcaption>i_start = 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_3.png" alt="Edit starting at 3 steps">
          <figcaption>i_start = 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_5.png" alt="Edit starting at 5 steps">
          <figcaption>i_start = 5</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_7.png" alt="Edit starting at 7 steps">
          <figcaption>i_start = 7</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_10.png" alt="Edit starting at 10 steps">
          <figcaption>i_start = 10</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_20.png" alt="Edit starting at 20 steps">
          <figcaption>i_start = 20</figcaption>
        </figure>
      </div>

      <p class="discussion">
        As the starting index <code>i_start</code> increases (meaning we add more noise before
        denoising), the output gradually drifts away from the original. Low values like
        <code>i_start = 1</code> mostly perform mild cleanup, while high values such as
        <code>i_start = 20</code> produce a new image that shares only loose structure and color
        palette with the input.
      </p>

      <!-- DC photo edits -->
      <h4>Edits of My Own Image 1 — Washington, D.C.</h4>
      <p class="summary">
        I repeat the same SDEdit procedure for a personal photo taken in Washington, D.C.
        The first row shows the original image and its noisy-start edits at different
        <code>i_start</code> values.
      </p>

      <div class="grid seven">
        <figure class="card">
          <img src="./media/part_c/2.3/dc_original.png" alt="Original DC image (rescaled)">
          <figcaption>Rescaled DC original.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_1.png" alt="DC edit i_start=1">
          <figcaption>i_start = 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_3.png" alt="DC edit i_start=3">
          <figcaption>i_start = 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_5.png" alt="DC edit i_start=5">
          <figcaption>i_start = 5</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_7.png" alt="DC edit i_start=7">
          <figcaption>i_start = 7</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_10.png" alt="DC edit i_start=10">
          <figcaption>i_start = 10</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_20.png" alt="DC edit i_start=20">
          <figcaption>i_start = 20</figcaption>
        </figure>
      </div>

      <div class="grid two">
        <figure class="card">
          <img src="./media/part_c/2.3/dc.jpeg" alt="Original full-resolution DC photo">
          <figcaption>Original full-resolution DC photo.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_fake.png" alt="DC SDEdit + upsampled to 256x256">
          <figcaption>
            Final SDEdit result upsampled to 256×256 via Stage&nbsp;2 (<code>dc_fake.png</code>).
          </figcaption>
        </figure>
      </div>

      <!-- Fuji photo edits -->
      <h4>Edits of My Own Image 2 — Mount Fuji</h4>
      <p class="summary">
        I also apply SDEdit to a photo of Mount Fuji. Again, higher <code>i_start</code> values
        encourage the model to reinterpret the scene while keeping the overall composition.
      </p>

      <div class="grid seven">
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_original.png" alt="Original Fuji image (rescaled)">
          <figcaption>Rescaled Fuji original.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_1.png" alt="Fuji edit i_start=1">
          <figcaption>i_start = 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_3.png" alt="Fuji edit i_start=3">
          <figcaption>i_start = 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_5.png" alt="Fuji edit i_start=5">
          <figcaption>i_start = 5</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_7.png" alt="Fuji edit i_start=7">
          <figcaption>i_start = 7</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_10.png" alt="Fuji edit i_start=10">
          <figcaption>i_start = 10</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_20.png" alt="Fuji edit i_start=20">
          <figcaption>i_start = 20</figcaption>
        </figure>
      </div>

      <div class="grid two">
        <figure class="card">
          <img src="./media/part_c/2.3/fuji.JPG" alt="Original full-resolution Fuji photo">
          <figcaption>Original full-resolution Fuji photo.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_fake.png" alt="Fuji SDEdit + upsampled to 256x256">
          <figcaption>
            Final SDEdit result upsampled to 256×256 via Stage&nbsp;2 (<code>fuji_fake.png</code>).
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        On both personal photos, low-noise edits act like a learned “photo filter,” cleaning up
        noise while preserving most structure. As the noise level increases, the model keeps the
        rough layout (sky vs. ground, horizon lines, silhouettes) but freely invents textures,
        lighting, and fine details. Upsampling the 64×64 outputs with the Stage&nbsp;2 model
        produces sharper 256×256 images that look more like stylized paintings of the original
        scenes than literal reconstructions.
      </p>
    </section>


        <!-- ===================== Part 3 ===================== -->
    <section id="part3">
      <h2>Part 3 — Visual Anagrams</h2>
      <p class="summary">
        In the final part of the project, I implement the <em>Visual Anagrams</em> technique
        (CVPR&nbsp;2024). The goal is to create a single image that looks like one subject when
        upright, but reveals a completely different subject when flipped upside down.
        This is achieved by denoising an image using two different prompts simultaneously:
        one with the upright image, and another with a vertically flipped version of the same
        noisy image. Their predicted noise estimates are averaged together at each diffusion
        step, producing an “ambiguous” image that resolves differently depending on orientation.
      </p>

      <h3>3.1 Old Man and Campfire Illusion</h3>
      <p class="summary">
        I use the prompts <em>"an oil painting of an old man"</em> and
        <em>"an oil painting of people around a campfire"</em>.
        The model begins from pure noise and iteratively denoises using
        orientation-specific noise predictions. The result is a single image that
        changes identity depending on its orientation.
      </p>

      <div class="grid two">
        <figure class="card">
          <img src="./media/part_d/old_man.png" alt="Visual anagram upright (old man)">
          <figcaption>Upright — Appears as an old man.</figcaption>
        </figure>

        <figure class="card">
          <img src="./media/part_d/fire.png" alt="Visual anagram flipped (campfire)">
          <figcaption>Flipped — Appears as a campfire scene.</figcaption>
        </figure>
      </div>

      <p class="discussion">
        The illusion works because the model learns to encode features supporting both prompts
        simultaneously. When upright, facial structure and lighting cues dominate, producing the
        impression of an old man. When the image is flipped, the same patterns reinterpret as a
        campfire glow, figures, and surrounding scenery. This dual-interpretation property is a
        direct result of averaging the two denoising paths during sampling.
      </p>

      <p class="summary">
        Although only one illusion is required for this project, the same method can be used
        to create many other reversible images with different pairings of artistic prompts.
      </p>
    </section>



  </main>

  <footer class="site-footer">
    <p>© <span id="year"></span> Compton Ross</p>
  </footer>

  <!-- Zoom modal -->
  <div id="imgModal" class="modal">
    <span id="modalClose" class="close">&times;</span>
    <img class="modal-content" id="modalImg" alt="">
    <div id="caption"></div>
  </div>

  <script>
    // Click-to-zoom behavior
    const modal = document.getElementById("imgModal");
    const modalImg = document.getElementById("modalImg");
    const captionText = document.getElementById("caption");
    const closeBtn = document.getElementById("modalClose");

    document.querySelectorAll("section img").forEach(img => {
      img.style.cursor = "zoom-in";
      img.addEventListener("click", () => {
        modal.style.display = "block";
        modalImg.src = img.src;
        captionText.textContent =
          img.nextElementSibling ? img.nextElementSibling.innerText : "";
      });
    });

    closeBtn.addEventListener("click", () => {
      modal.style.display = "none";
    });

    modal.addEventListener("click", (e) => {
      if (e.target === modal) modal.style.display = "none";
    });

    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
