<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Project 4 — Diffusion Models</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="./style.css">
</head>
<body>
  <header class="site-header">
    <div class="header-inner">
      <div class="brand">
        <a class="back" href="../">&larr; Home</a>
        <h1>Project 4 — Diffusion Models</h1>
        <p class="byline">Compton Ross</p>
      </div>
      <nav class="toc">
        <a href="#part0">Part 0</a>
        <a href="#part1">Part 1</a>
        <a href="#part2">Part 2</a>
        <a href="#part3">Part 3</a>

      </nav>
    </div>
  </header>

  <main>
        <!-- ===================== Part 0 ===================== -->
    <section id="part0">
      <h2>Part 0 — Setup</h2>
      <p class="summary">
        I set up the DeepFloyd IF pipeline on Hugging Face, loaded the provided
        prompt embeddings, and generated images for three given prompts:
        <em>"a rocket ship"</em>,
        <em>"an oil painting of a snowy mountain village"</em>, and
        <em>"a man wearing a hat"</em>.
        For each prompt I compare samples generated with a small number of denoising steps
        (<code>num_inference_steps = 5</code>) and a larger number of steps
        (<code>num_inference_steps = 20</code>) using the same random seed (180).
      </p>

      <!-- ===== Rocket Ship Samples ===== -->
      <h3>Rocket Ship</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/rocket_5.png" alt="Rocket ship with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The rocket is visible, but the image grainy and still has a lot of noise.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/rocket_20.png" alt="Rocket ship with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> The rocket is much more clear, and the overall image is much more crisp.
          </figcaption>
        </figure>
      </div>

      <!-- ===== Snowy Village Samples ===== -->
      <h3>Snowy Mountain Village</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/oil_painting_5.png" alt="Snowy village with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The village is extremely blurry, and it is hard to make out any fine details.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/oil_painting_20.png" alt="Snowy village with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> All parts of the image are very sharp and are stylized. You can easily see many fine details of the painting.
        </figure>
      </div>

      <!-- ===== Man Wearing a Hat Samples ===== -->
      <h3>Man Wearing a Hat</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/man_5.png" alt="Man in hat with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The portrait is very stylized and is extremely grainy other than the man's face.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/man_20.png" alt="Man in hat with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> All parts of the image are very realistic.
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        Across all three prompts, using only <code>5</code> denoising steps produces quick but noisy
        outputs. Increasing to
        <code>20</code> steps substantially improves the image, but at the
        cost of additional compute time. 
      </p>
    </section>


        <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2>Part 1 — Forward &amp; Reverse Processes</h2>
      <p class="summary">
        Part&nbsp;1 focuses on how diffusion models add and remove noise. I first implement the
        forward noising process, then compare traditional Gaussian denoising to a learned UNet,
        and finally run an iterative reverse process.
      </p>
    </section>

    <!-- ===== Part 1.1 — Forward Process ===== -->
    <section id="part1-forward">
      <h3>1.1 Forward Process (Adding Noise)</h3>
      <p class="summary">
        The forward process corrupts a clean image <code>x₀</code> into a noisy version
        <code>x<sub>t</sub></code> according to
        <code>x<sub>t</sub> = √&alpha;̄<sub>t</sub>x₀ + √(1−&alpha;̄<sub>t</sub>)&epsilon;</code>.
        Below I show the original image and its noised versions at
        <code>t = 250, 500, 750</code> using the provided noise schedule
        <code>alphas_cumprod</code>.
      </p>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original test image">
          <figcaption>Original test image <code>x₀</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code></figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption><code>t = 500</code></figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption><code>t = 750</code></figcaption>
        </figure>
      </div>
      <p class="discussion">
        As <code>t</code> increases, the signal is both rescaled and mixed with noise. At medium
        timesteps the global structure is barely visible, and by <code>t = 750</code> the image is almost unrecognizable.
      </p>
    </section>

    <!-- ===== Part 1.2 — Gaussian Denoising ===== -->
    <section id="part1-gaussian">
      <h3>1.2 Traditional Denoising (Gaussian Blur)</h3>
      <p class="summary">
        I attempted to recover the original image from the noisy versions using Gaussian blur.
        For each timestep I show the noisy input and
        the best Gaussian-denoised result I could obtain by tuning the blur parameters.
      </p>

      <h4>t = 250</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_250.png" alt="Gaussian denoised t=250">
          <figcaption>
            Gaussian denoised at <code>t = 250</code>
          </figcaption>
        </figure>
      </div>

      <h4>t = 500</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption>Noisy image at <code>t = 500</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_500.png" alt="Gaussian denoised t=500">
          <figcaption>
            Gaussian denoised at <code>t = 500</code>
          </figcaption>
        </figure>
      </div>

      <h4>t = 750</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption>Noisy image at <code>t = 750</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_750.png" alt="Gaussian denoised t=750">
          <figcaption>
            Gaussian denoised at <code>t = 750</code>
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        Gaussian blur can reduce high-frequency noise, but it has no knowledge of natural image
        statistics. At very low noise levels it might produce a somewhat passable result, but at higher timesteps it
        simply smooths everything. For any of these examples, a diffusion model would be necessary, as none of the results I would deem passable.
        
      </p>
    </section>

    <!-- ===== Part 1.3 — One-Step UNet Denoising ===== -->
    <section id="part1-unet">
      <h3>1.3 One-Step Denoising</h3>
      <p class="summary">
        I next use the pretrained Stage&nbsp;1 UNet to predict the noise
        in <code>x<sub>t</sub></code> and reconstruct an estimate of the clean image via
        <code>ĥx₀ = (x<sub>t</sub> − √(1−&alpha;̄<sub>t</sub>) ĥ&epsilon;)/√&alpha;̄<sub>t</sub></code>.
        For each timestep I show the original, the noisy input, and the one-step denoised result.
      </p>

      <h4>t = 250</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=250">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_250.png" alt="One-step denoised t=250">
          <figcaption>One-step denoised result at <code>t = 250</code>.</figcaption>
        </figure>
      </div>

      <h4>t = 500</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=500">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption>Noisy image at <code>t = 500</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_500.png" alt="One-step denoised t=500">
          <figcaption>One-step denoised result at <code>t = 500</code>.</figcaption>
        </figure>
      </div>

      <h4>t = 750</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=750">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption>Noisy image at <code>t = 750</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_750.png" alt="One-step denoised t=750">
          <figcaption>One-step denoised result at <code>t = 750</code>.</figcaption>
        </figure>
      </div>

      <p class="discussion">
        At moderate noise levels (e.g., <code>t = 250</code>), a single UNet pass produces a
        decently accurate reconstruction. As the noise level increases, the one-step estimate
        still produces a plausible natural image but drifts further away from the exact original,
        also causing the image to be extremely blurry, still justifying a need for multiple steps.
      </p>
    </section>

    <!-- ===== Part 1.4 — Iterative Denoising ===== -->
    <section id="part1-iter">
      <h3>1.4 Iterative Denoising</h3>
      <p class="summary">
        Finally, I implement an iterative reverse diffusion process using a strided schedule of
        timesteps. Starting from a very noisy image at <code>t = 690</code>, the model repeatedly
        predicts noise and steps toward <code>t = 0</code>. Below are snapshots of the image every 5 iterations.
      </p>

      <div class="grid six">
        <figure class="card">
          <img src="./media/part_b/iter_0.png" alt="Iterative denoise step 0">
          <figcaption>Iteration 0 (starting from <code>t = 690</code>)</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_5.png" alt="Iterative denoise step 5">
          <figcaption>Iteration 5</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_10.png" alt="Iterative denoise step 10">
          <figcaption>Iteration 10</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_15.png" alt="Iterative denoise step 15">
          <figcaption>Iteration 15</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_20.png" alt="Iterative denoise step 20">
          <figcaption>Iteration 20</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_22.png" alt="Iterative denoise final">
          <figcaption>Iteration 22</figcaption>
        </figure>
      </div>

      <h4>Final Comparison at t = 690</h4>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image">
          <figcaption>Original clean image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_690.png" alt="Noisy image t=690">
          <figcaption>Noisy image at <code>t = 690</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iterative_690.png" alt="Iterative denoise result">
          <figcaption>Iterative denoising result.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_690.png" alt="One-step denoise result">
          <figcaption>One-step denoise from <code>t = 690</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_690.png" alt="Gaussian blur result">
          <figcaption>Gaussian blur baseline from <code>t = 690</code>.</figcaption>
        </figure>
      </div>

      <p class="discussion">
        The sequence of iterated images shows the diffusion model gradually improving as steps progress. Compared to the one-step and Gaussian blur, iterative
        denoising produces the cleanest and most coherent result, especially when starting from a
        very noisy state. The Gaussian version is overly smooth and loses structure, while the
        one-step tends to miss out on more fine details (as seen with the trees and surrounding structure).
      </p>
    </section>


        <!-- ===================== Part 2 ===================== -->
    <section id="part2">
      <h2>Part 2 — Understanding the Forward & Reverse Processes</h2>
      <p class="summary">
        In Part&nbsp;2 I use the diffusion model generatively. First I sample images from
        random noise, then I apply classifier-free guidance (CFG) to boost image quality, and
        finally I perform image-to-image translation on both a provided test image and
        two of my own photos.
      </p>
    </section>

    <!-- ===== Part 2.1 — Diffusion Model Sampling ===== -->
    <section id="part2-sampling">
      <h3>2.1 Diffusion Model Sampling from Noise</h3>
      <p class="summary">
        Using the <code>iterative_denoise</code> routine from Part&nbsp;1, I start from pure
        Gaussian noise and denoise all the way to <code>t = 0</code> with the prompt
        <em>"a high quality photo"</em>. The model gradually transforms random noise into coherent
        images.
      </p>

      <div class="grid five">
        <figure class="card">
          <img src="./media/part_c/2.1/sample_1.png" alt="Sampled image 1">
          <figcaption>Sample 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.1/sample_2.png" alt="Sampled image 2">
          <figcaption>Sample 2</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.1/sample_3.png" alt="Sampled image 3">
          <figcaption>Sample 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.1/sample_4.png" alt="Sampled image 4">
          <figcaption>Sample 4</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.1/sample_5.png" alt="Sampled image 5">
          <figcaption>Sample 5</figcaption>
        </figure>
      </div>

      <p class="discussion">
        Even without explicit class labels, the model generates very good images from pure noise. Some samples are more coherent than others, but overall the textures,
        lighting, and global structure help show how good the model is.
      </p>
    </section>

    <!-- ===== Part 2.2 — Classifier-Free Guidance (CFG) ===== -->
    <section id="part2-cfg">
      <h3>2.2 Classifier-Free Guidance (CFG)</h3>
      <p class="summary">
        Next I implement <code>iterative_denoise_cfg</code>, which combines conditional and
        unconditional noise predictions using classifier free guidance. I use the same
        <em>"a high quality photo"</em> prompt but now apply a guidance scale of
        <code>&gamma; = 7</code> to sharpen the samples.
      </p>

      <div class="grid five">
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_1.png" alt="CFG sample 1">
          <figcaption>CFG Sample 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_2.png" alt="CFG sample 2">
          <figcaption>CFG Sample 2</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_3.png" alt="CFG sample 3">
          <figcaption>CFG Sample 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_4.png" alt="CFG sample 4">
          <figcaption>CFG Sample 4</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.2/cfg_sample_5.png" alt="CFG sample 5">
          <figcaption>CFG Sample 5</figcaption>
        </figure>
      </div>

      <p class="discussion">
        Compared to the unguided samples, CFG images are noticeably different, but I would not call them better.
        It appears as though the images are much more vivid, and tend to focus on lighting and color more than anything else. 
      </p>
    </section>

    <!-- ===== Part 2.3 — Image-to-Image Translation (SDEdit) ===== -->
    <section id="part2-sdedit">
      <h3>2.3 Image-to-Image Translation</h3>
      <p class="summary">
        Lastly, I use the image to image translation procedure: start from a real image, add noise to a chosen
        timestep, and then run CFG denoising back to <code>t = 0</code>. With low noise, the output
        should stay close to the original. With high noise, the model sort of “reimagines” the scene while
        preserving some of the main focal points.
      </p>

      <!-- Provided test image edits -->
      <h4>Edits of the Provided Test Image</h4>
      <div class="grid seven">
        <figure class="card">
          <img src="./media/part_c/2.3/original.png" alt="Original test image for SDEdit">
          <figcaption>Original test image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_1.png" alt="Edit starting at 1 step">
          <figcaption>i_start = 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_3.png" alt="Edit starting at 3 steps">
          <figcaption>i_start = 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_5.png" alt="Edit starting at 5 steps">
          <figcaption>i_start = 5</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_7.png" alt="Edit starting at 7 steps">
          <figcaption>i_start = 7</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_10.png" alt="Edit starting at 10 steps">
          <figcaption>i_start = 10</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/i_20.png" alt="Edit starting at 20 steps">
          <figcaption>i_start = 20</figcaption>
        </figure>
      </div>

      <p class="discussion">
        As the starting index <code>i_start</code> increases it appears the output gets closer to the original. Low values like
        <code>i_start = 1</code> seem to perform abstractions on the image, while
        <code>i_start = 20</code> seems create a similar image to the original.
      </p>

      <!-- DC photo edits -->
      <h4>Edits of My Own Image 1 — Washington, D.C.</h4>
      <p class="summary">
        I repeat the same procedure for a personal photo taken in Washington, D.C while on a conference visit with some classmates.
        The first row shows the original image and its noisy-start edits at different
        <code>i_start</code> values.
      </p>

      <div class="grid seven">
        <figure class="card">
          <img src="./media/part_c/2.3/dc_original.png" alt="Original DC image (rescaled)">
          <figcaption>Rescaled DC original.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_1.png" alt="DC edit i_start=1">
          <figcaption>i_start = 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_3.png" alt="DC edit i_start=3">
          <figcaption>i_start = 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_5.png" alt="DC edit i_start=5">
          <figcaption>i_start = 5</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_7.png" alt="DC edit i_start=7">
          <figcaption>i_start = 7</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_10.png" alt="DC edit i_start=10">
          <figcaption>i_start = 10</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_20.png" alt="DC edit i_start=20">
          <figcaption>i_start = 20</figcaption>
        </figure>
      </div>

      <div class="grid two">
        <figure class="card">
          <img src="./media/part_c/2.3/dc.jpeg" alt="Original full-resolution DC photo">
          <figcaption>Original DC photo.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/dc_fake.png" alt="DC SDEdit + upsampled to 256x256">
          <figcaption>
            Upscaled final result.
          </figcaption>
        </figure>
      </div>

      <!-- Fuji photo edits -->
      <h4>Edits of My Own Image 2 — Mount Fuji</h4>
      <p class="summary">
        I also applied it to a photo of my girlfriend and I in front of Mount Fuji during our visit to Japan.
      </p>

      <div class="grid seven">
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_original.png" alt="Original Fuji image (rescaled)">
          <figcaption>Rescaled Fuji original.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_1.png" alt="Fuji edit i_start=1">
          <figcaption>i_start = 1</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_3.png" alt="Fuji edit i_start=3">
          <figcaption>i_start = 3</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_5.png" alt="Fuji edit i_start=5">
          <figcaption>i_start = 5</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_7.png" alt="Fuji edit i_start=7">
          <figcaption>i_start = 7</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_10.png" alt="Fuji edit i_start=10">
          <figcaption>i_start = 10</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_20.png" alt="Fuji edit i_start=20">
          <figcaption>i_start = 20</figcaption>
        </figure>
      </div>

      <div class="grid two">
        <figure class="card">
          <img src="./media/part_c/2.3/fuji.JPG" alt="Original full-resolution Fuji photo">
          <figcaption>Original full-resolution Fuji photo.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_c/2.3/fuji_fake.png" alt="Fuji SDEdit + upsampled to 256x256">
          <figcaption>
            Upscaled final result.
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        On both personal photos, it once again appears that lower values of i_start seem to make the results more abstract and further from the originals.
        Although the final result for the DC image is questionable, by i_start = 20 for the Mt. Fuji image, the model seems to reconstruct and image of a couple standing in front of a mountain.
        The first few images however seem to have nothing to do with the original.
      </p>
    </section>


        <!-- ===================== Part 3 ===================== -->
    <section id="part3">
      <h2>Part 3 — Visual Anagrams</h2>
      <p class="summary">
        In the final part of the project, I implement the <em>Visual Anagrams</em> technique
        (CVPR&nbsp;2024). The goal wwas to create a single image that looks like one subject when
        upright, but something else when flipped upside down.
        This is achieved by denoising an image using two different prompts simultaneously.
        One with the upright image, and another with a vertically flipped version of the same
        noisy image. Their predicted noise estimates are averaged together at each diffusion
        step, producing an “ambiguous” image that resolves differently depending on orientation.
      </p>

      <h3>3.1 Old Man and Campfire Illusion</h3>
      <p class="summary">
        I use the prompts <em>"an oil painting of an old man"</em> and
        <em>"an oil painting of people around a campfire"</em>.
        The model begins from pure noise and iteratively denoises using
        orientation-specific noise predictions. The result is a single image that
        changes identity depending on its orientation.
      </p>

      <div class="grid two">
        <figure class="card">
          <img src="./media/part_d/old_man.png" alt="Visual anagram upright (old man)">
          <figcaption>Upright — Appears as an old man.</figcaption>
        </figure>

        <figure class="card">
          <img src="./media/part_d/fire.png" alt="Visual anagram flipped (campfire)">
          <figcaption>Flipped — Appears as a campfire scene. (with silhouette behind)</figcaption>
        </figure>
      </div>

      <p class="discussion">
        The illusion works because the model learns to encode features supporting both prompts
        simultaneously. When upright, facial structure and lighting cues dominate, producing the
        impression of an old man. When the image is flipped, the same patterns reinterpret as a
        campfire glow with rising smoke and the silhouette of a person in the background. This property is a
        direct result of averaging the two denoising paths during sampling.
      </p>

      
    </section>



  </main>

  <footer class="site-footer">
    <p>© <span id="year"></span> Compton Ross</p>
  </footer>

  <!-- Zoom modal -->
  <div id="imgModal" class="modal">
    <span id="modalClose" class="close">&times;</span>
    <img class="modal-content" id="modalImg" alt="">
    <div id="caption"></div>
  </div>

  <script>
    // Click-to-zoom behavior
    const modal = document.getElementById("imgModal");
    const modalImg = document.getElementById("modalImg");
    const captionText = document.getElementById("caption");
    const closeBtn = document.getElementById("modalClose");

    document.querySelectorAll("section img").forEach(img => {
      img.style.cursor = "zoom-in";
      img.addEventListener("click", () => {
        modal.style.display = "block";
        modalImg.src = img.src;
        captionText.textContent =
          img.nextElementSibling ? img.nextElementSibling.innerText : "";
      });
    });

    closeBtn.addEventListener("click", () => {
      modal.style.display = "none";
    });

    modal.addEventListener("click", (e) => {
      if (e.target === modal) modal.style.display = "none";
    });

    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
