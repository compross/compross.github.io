<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Project 4 — Diffusion Models</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="./style.css">
</head>
<body>
  <header class="site-header">
    <div class="header-inner">
      <div class="brand">
        <a class="back" href="../">&larr; Home</a>
        <h1>Project 4 — Diffusion Models</h1>
        <p class="byline">Compton Ross</p>
      </div>
      <nav class="toc">
        <a href="#part0">Part 0</a>
        <a href="#part1">Part 1</a>
        <a href="#part2">Part 2</a>
        <a href="#part3">Part 3</a>

      </nav>
    </div>
  </header>

  <main>
        <!-- ===================== Part 0 ===================== -->
    <section id="part0">
      <h2>Part 0 — Setup & Effect of Sampling Steps</h2>
      <p class="summary">
        In Part&nbsp;0 I set up the DeepFloyd IF pipeline on Hugging Face, loaded the provided
        prompt embeddings, and generated images for the three given prompts:
        <em>"a rocket ship"</em>,
        <em>"an oil painting of a snowy mountain village"</em>, and
        <em>"a man wearing a hat"</em>.
        For each prompt I compare samples generated with a small number of denoising steps
        (<code>num_inference_steps = 5</code>) and a larger number of steps
        (<code>num_inference_steps = 20</code>) using the same random seed.
        This highlights the speed–quality trade-off in diffusion sampling.
      </p>

      <!-- ===== Rocket Ship Samples ===== -->
      <h3>Rocket Ship — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/rocket_5.png" alt="Rocket ship with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The overall rocket shape is visible, but the scene is very noisy
            and grainy. Background structure is muddled and fine details are poorly defined.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/rocket_20.png" alt="Rocket ship with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> The rocket is crisp with clean edges, smooth gradients, and
            clear light sources. Increasing the number of steps greatly improves coherence and detail.
          </figcaption>
        </figure>
      </div>

      <!-- ===== Snowy Village Samples ===== -->
      <h3>Snowy Mountain Village — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/oil_painting_5.png" alt="Snowy village with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The village is only loosely defined; buildings and trees blur
            into a textured pointillist pattern. The scene feels abstract and lacks clear depth.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/oil_painting_20.png" alt="Snowy village with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> Houses, trees, and snow banks are sharp and stylized, with
            strong color contrasts and readable geometry. Longer sampling recovers a much more
            coherent landscape.
          </figcaption>
        </figure>
      </div>

      <!-- ===== Man Wearing a Hat Samples ===== -->
      <h3>Man Wearing a Hat — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/man_5.png" alt="Man in hat with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The portrait is heavily textured and stylized, with facial
            features still recognizable but soft and noisy. The image looks more like a rough
            pointillist painting.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/man_20.png" alt="Man in hat with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> The face, hat, and clothing are sharply defined and realistic.
            Noise is largely removed, and lighting and shading look much more natural.
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        Across all three prompts, using only <code>5</code> denoising steps produces quick but noisy
        outputs with blurred edges and unstable structure. Increasing to
        <code>20</code> steps substantially improves global coherence and fine detail, but at the
        cost of additional compute time. For the remainder of the project I use a moderate number of
        sampling steps to balance runtime with visual quality.
      </p>
    </section>


    <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2>Part 1 — Forward &amp; Reverse Processes</h2>
      <p class="summary">
        Part&nbsp;1 focuses on understanding how diffusion models add and remove noise. I implement
        the forward noising process, experiment with traditional Gaussian blur denoising, and then use
        a pretrained UNet and an iterative sampler to recover images from increasingly noisy inputs.
      </p>
    </section>

    <!-- ===== Part 1.1 — Forward Process ===== -->
    <section id="part1-forward">
      <h3>1.1 Forward Process (Adding Noise)</h3>
      <p class="summary">
        I implement the forward process
        <code>x<sub>t</sub> = √&alpha;̄<sub>t</sub>x<sub>0</sub> + √(1−&alpha;̄<sub>t</sub>)&epsilon;</code>,
        where &alpha;̄<sub>t</sub> is taken from the prescribed noise schedule. The test image is
        progressively degraded at timesteps <code>t = 250, 500, 750</code>.
      </p>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part1/forward_original.png" alt="Original test image">
          <figcaption>Original test image <code>x₀</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/forward_t250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code> (moderate noise, structure still visible).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/forward_t500.png" alt="Noisy image t=500">
          <figcaption>Noisy image at <code>t = 500</code> (heavier corruption, edges fading).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/forward_t750.png" alt="Noisy image t=750">
          <figcaption>Noisy image at <code>t = 750</code> (image is close to pure noise).</figcaption>
        </figure>
      </div>
      <p class="discussion">
        As expected, smaller timesteps preserve high-level content and edges, while larger timesteps
        push the image toward an almost Gaussian noise field. The forward process is not just additive
        noise: the signal is also scaled by <code>√&alpha;̄<sub>t</sub></code>, which is important
        for the later reverse step where we attempt to reconstruct <code>x₀</code>.
      </p>
    </section>

    <!-- ===== Part 1.2 — Gaussian Denoising ===== -->
    <section id="part1-gaussian">
      <h3>1.2 Traditional Denoising (Gaussian Blur)</h3>
      <p class="summary">
        I apply Gaussian blur to the noisy images at <code>t = 250, 500, 750</code>, tuning kernel
        size and <code>σ</code> separately for each noise level to get the best possible restoration.
      </p>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part1/gaussian_t250.png" alt="Gaussian denoise t=250">
          <figcaption>Gaussian denoising at <code>t = 250</code>; noise reduced but edges softened.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/gaussian_t500.png" alt="Gaussian denoise t=500">
          <figcaption>Gaussian denoising at <code>t = 500</code>; textures heavily blurred.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/gaussian_t750.png" alt="Gaussian denoise t=750">
          <figcaption>Gaussian denoising at <code>t = 750</code>; image mostly becomes a smooth color field.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        Gaussian filtering can smooth out high-frequency noise, but it does not know anything about
        natural image structure. At low noise levels it gives a mildly usable result, but at higher
        levels the denoised images are overly smooth and lose most semantic content. This highlights
        why diffusion models need learned denoisers instead of relying purely on classical filters.
      </p>
    </section>

    <!-- ===== Part 1.3 — One-Step UNet Denoising ===== -->
    <section id="part1-unet">
      <h3>1.3 One-Step Denoising with a Pretrained UNet</h3>
      <p class="summary">
        Next, I use the pretrained Stage&nbsp;1 UNet to predict the noise
        <code>ĥ&epsilon;</code> for noisy images and then reconstruct
        <code>ĥx₀ = (x<sub>t</sub> − √(1 − &alpha;̄<sub>t</sub>) ĥ&epsilon;)/√&alpha;̄<sub>t</sub></code>.
        This effectively performs a single reverse diffusion step conditioned on the prompt
        <em>"a high quality photo"</em>.
      </p>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part1/unet_original.png" alt="Original image for UNet denoising">
          <figcaption>Original test image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/unet_noisy_t500.png" alt="Noisy image for UNet denoising">
          <figcaption>Noisy image at <code>t = 500</code> fed into the UNet.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/unet_clean_est_t500.png" alt="UNet denoised estimate">
          <figcaption>One-step UNet estimate of <code>x₀</code> from <code>t = 500</code>.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        Even with a single denoising step, the UNet does a surprisingly good job reconstructing a
        plausible image. At moderate noise levels the model can largely restore structure and textures,
        although fine details and colors may drift away from the original. At very high noise levels,
        the reconstruction looks more like a “hallucinated” image from the same distribution rather than
        a faithful recovery of the exact input.
      </p>
    </section>

    <!-- ===== Part 1.4 — Iterative Denoising ===== -->
    <section id="part1-iter">
      <h3>1.4 Iterative Denoising with Strided Timesteps</h3>
      <p class="summary">
        Finally, I implement an iterative denoising procedure that steps through a strided set of
        timesteps from <code>t = 990</code> down to <code>t = 0</code>. At each step, I use the UNet
        to estimate the noise and then apply a DDPM-style update rule, including learned variance
        from the scheduler.
      </p>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part1/iter_step0.png" alt="Iterative denoise early step">
          <figcaption>Early denoising step (very noisy, structure just starting to emerge).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/iter_step5.png" alt="Iterative denoise mid step">
          <figcaption>After several steps; shapes and rough layout become visible.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/iter_step10.png" alt="Iterative denoise later step">
          <figcaption>Later step; most high-level content is recognizable.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/iter_step15.png" alt="Iterative denoise near end">
          <figcaption>Near the end of the schedule; details are refined.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/iter_final.png" alt="Final iterative denoise result">
          <figcaption>Final <code>x₀</code> estimate from iterative denoising.</figcaption>
        </figure>
      </div>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part1/iter_vs_one_step.png" alt="Iterative vs one-step comparison">
          <figcaption>Iterative denoising vs. one-step UNet reconstruction.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/iter_vs_gaussian.png" alt="Iterative vs Gaussian blur">
          <figcaption>Iterative denoising vs. Gaussian blur baseline.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part1/noisy_t_strided.png" alt="Initial noisy image at strided timestep">
          <figcaption>Noisy starting point at <code>t =</code> strided_timesteps[10].</figcaption>
        </figure>
      </div>
      <p class="discussion">
        Showing the image every 5th loop illustrates how the diffusion model gradually pushes the
        sample back toward the natural image manifold. The iterative sampler produces much more
        coherent and detailed images than the one-step reconstruction, particularly when starting
        from highly corrupted inputs. Compared to Gaussian blur, the diffusion-based method preserves
        edges, textures, and semantic structure while still removing noise.
      </p>
    </section>

    <!-- ===================== Part 2 ===================== -->
    <section id="part2">
      <h2>Part 2 — Sampling, Classifier-Free Guidance, and SDEdit</h2>
      <p class="summary">
        Part&nbsp;2 uses the iterative sampler to generate images from pure noise, adds classifier-free
        guidance (CFG) to improve visual quality, and then applies the model to image-to-image
        translation via SDEdit, including edits on my own photographs.
      </p>
    </section>

    <!-- ===== Part 2.1 — Sampling from Noise ===== -->
    <section id="part2-sampling">
      <h3>2.1 Sampling from Random Noise</h3>
      <p class="summary">
        Starting from random Gaussian noise at the Stage&nbsp;1 resolution, I run the
        <code>iterative_denoise</code> pipeline with <em>"a high quality photo"</em> as the text
        condition. This produces unconditional samples from the learned image distribution.
      </p>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part2/sampling_sample1.png" alt="Sample 1 from noise">
          <figcaption>Sample 1 generated from pure noise.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sampling_sample2.png" alt="Sample 2 from noise">
          <figcaption>Sample 2 generated from pure noise.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sampling_sample3.png" alt="Sample 3 from noise">
          <figcaption>Sample 3 generated from pure noise.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sampling_sample4.png" alt="Sample 4 from noise">
          <figcaption>Sample 4 generated from pure noise.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sampling_sample5.png" alt="Sample 5 from noise">
          <figcaption>Sample 5 generated from pure noise.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        Without guidance, the samples are diverse but sometimes lack crisp structure or clear focal
        points. This is expected: the model is encouraged to match the overall data distribution, not
        any specific concept. In the next section, classifier-free guidance trades some diversity for
        more visually striking images.
      </p>
    </section>

    <!-- ===== Part 2.2 — Classifier-Free Guidance ===== -->
    <section id="part2-cfg">
      <h3>2.2 Classifier-Free Guidance (CFG)</h3>
      <p class="summary">
        I implement <code>iterative_denoise_cfg</code>, which combines conditional and unconditional
        noise estimates <code>&epsilon<sub>c</sub></code> and <code>&epsilon<sub>u</sub></code> as
        <code>&epsilon = &epsilon<sub>u</sub> + γ(&epsilon<sub>c</sub> − &epsilon<sub>u</sub>)</code>.
        I then sample images for <em>"a high quality photo"</em> with guidance scale
        <code>γ = 7</code>.
      </p>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part2/cfg_sample1.png" alt="CFG sample 1">
          <figcaption>CFG sample 1 with <code>γ = 7</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/cfg_sample2.png" alt="CFG sample 2">
          <figcaption>CFG sample 2 with <code>γ = 7</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/cfg_sample3.png" alt="CFG sample 3">
          <figcaption>CFG sample 3 with <code>γ = 7</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/cfg_sample4.png" alt="CFG sample 4">
          <figcaption>CFG sample 4 with <code>γ = 7</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/cfg_sample5.png" alt="CFG sample 5">
          <figcaption>CFG sample 5 with <code>γ = 7</code>.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        Compared to unguided sampling, CFG significantly sharpens edges and encourages stronger,
        more coherent global structure (for example, clearer subjects and backgrounds). However,
        high guidance values can sometimes over-constrain the model and reduce diversity or introduce
        saturation artifacts. I found <code>γ = 7</code> to be a good trade-off for this project.
      </p>
    </section>

    <!-- ===== Part 2.3 — Image-to-Image Translation (SDEdit) ===== -->
    <section id="part2-sdedit">
      <h3>2.3 Image-to-Image Translation with SDEdit</h3>
      <p class="summary">
        In this section I apply SDEdit: I add noise to existing images (the provided test image and
        two of my own photos) and then denoise them with <code>iterative_denoise_cfg</code>. By
        changing how many denoising steps we run (or equivalently, where we start in the schedule),
        we control how large the “edit” is: small edits preserve structure closely, while larger edits
        push the image toward a new but related sample.
      </p>
      <h4>Test Image Edits</h4>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part2/sdedit_test_original.png" alt="Original test image for SDEdit">
          <figcaption>Original test image (no noise).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_test_i1.png" alt="Test image edit i_start=1">
          <figcaption>Test image edit with <code>i_start = 1</code> (very light edit).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_test_i7.png" alt="Test image edit i_start=7">
          <figcaption><code>i_start = 7</code>; more creative drift while preserving composition.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_test_i20.png" alt="Test image edit i_start=20">
          <figcaption><code>i_start = 20</code>; large edits, image resembles a new sample.</figcaption>
        </figure>
      </div>

      <h4>Edits on My Own Photographs</h4>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part2/sdedit_dc_original.png" alt="My DC photo original">
          <figcaption>My own DC image (original).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_dc_i10.png" alt="My DC photo SDEdit result">
          <figcaption>DC image SDEdit result (<code>i_start = 10</code>), then upsampled to 256&times;256.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_fuji_original.png" alt="My Fuji photo original">
          <figcaption>My own Mount Fuji image (original).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_fuji_i10.png" alt="My Fuji photo SDEdit result">
          <figcaption>Fuji image SDEdit result (<code>i_start = 10</code>), then upsampled to 256&times;256.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        With small noise levels, the SDEdit outputs look like lightly stylized versions of the
        originals: lighting, textures, and minor details change while the global layout stays the
        same. As I increase the starting noise level, the model becomes more “creative” and the
        outputs eventually resemble new scenes that merely echo the composition of the inputs.
        On my own photos, this gives a neat way to transform real-world shots into painterly,
        model-generated interpretations.
      </p>
    </section>

    <!-- ===================== Part 3 ===================== -->
    <section id="part3">
      <h2>Part 3 — Visual Anagrams</h2>
      <p class="summary">
        For the graduate-level component, I implement visual anagrams based on the CVPR&nbsp;2024
        paper. The goal is to generate a single image that looks like one prompt when viewed
        upright and another when flipped upside down, by combining noise estimates from two
        differently conditioned UNet passes.
      </p>
    </section>

    <!-- ===== Part 3 — Old Man / Campfire Illusion ===== -->
    <section id="part3-anagram-main">
      <h3>Visual Anagram: Old Man &amp; Campfire</h3>
      <p class="summary">
        I construct an image that resembles <em>"an oil painting of an old man"</em> when upright
        and <em>"an oil painting of people around a campfire"</em> when flipped. At each diffusion
        step, I:
      </p>
      <ul>
        <li>Denoise <code>x<sub>t</sub></code> with prompt <code>p₁</code> (old man) to get <code>&epsilon₁</code>.</li>
        <li>Flip <code>x<sub>t</sub></code>, denoise with prompt <code>p₂</code> (campfire), and flip the output back to get <code>&epsilon₂</code>.</li>
        <li>Average the noise estimates <code>&epsilon = (&epsilon₁ + &epsilon₂)/2</code> and perform one reverse diffusion step.</li>
      </ul>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part3/anagram_oldman_upright_256.png" alt="Upright">
          <figcaption>Upright image of an old man.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part3/anagram_oldman_flipped_256.png" alt="Flipped">
          <figcaption>Flipped image of a man behind a fire.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        The illusion works because local features can be interpreted in two different ways depending
        on orientation. By averaging noise estimates conditioned on two prompts, the sampler finds
        a compromise that is consistent with both descriptions. Getting a strong illusion requires
        some trial-and-error: different random seeds and guidance strengths can produce images that
        favor one prompt more than the other.
      </p>
    </section>


  </main>

  <footer class="site-footer">
    <p>© <span id="year"></span> Compton Ross</p>
  </footer>

  <!-- Zoom modal -->
  <div id="imgModal" class="modal">
    <span id="modalClose" class="close">&times;</span>
    <img class="modal-content" id="modalImg" alt="">
    <div id="caption"></div>
  </div>

  <script>
    // Click-to-zoom behavior
    const modal = document.getElementById("imgModal");
    const modalImg = document.getElementById("modalImg");
    const captionText = document.getElementById("caption");
    const closeBtn = document.getElementById("modalClose");

    document.querySelectorAll("section img").forEach(img => {
      img.style.cursor = "zoom-in";
      img.addEventListener("click", () => {
        modal.style.display = "block";
        modalImg.src = img.src;
        captionText.textContent =
          img.nextElementSibling ? img.nextElementSibling.innerText : "";
      });
    });

    closeBtn.addEventListener("click", () => {
      modal.style.display = "none";
    });

    modal.addEventListener("click", (e) => {
      if (e.target === modal) modal.style.display = "none";
    });

    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
