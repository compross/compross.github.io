<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Project 4 — Diffusion Models</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="./style.css">
</head>
<body>
  <header class="site-header">
    <div class="header-inner">
      <div class="brand">
        <a class="back" href="../">&larr; Home</a>
        <h1>Project 4 — Diffusion Models</h1>
        <p class="byline">Compton Ross</p>
      </div>
      <nav class="toc">
        <a href="#part0">Part 0</a>
        <a href="#part1">Part 1</a>
        <a href="#part2">Part 2</a>
        <a href="#part3">Part 3</a>

      </nav>
    </div>
  </header>

  <main>
        <!-- ===================== Part 0 ===================== -->
    <section id="part0">
      <h2>Part 0 — Setup & Effect of Sampling Steps</h2>
      <p class="summary">
        In Part&nbsp;0 I set up the DeepFloyd IF pipeline on Hugging Face, loaded the provided
        prompt embeddings, and generated images for the three given prompts:
        <em>"a rocket ship"</em>,
        <em>"an oil painting of a snowy mountain village"</em>, and
        <em>"a man wearing a hat"</em>.
        For each prompt I compare samples generated with a small number of denoising steps
        (<code>num_inference_steps = 5</code>) and a larger number of steps
        (<code>num_inference_steps = 20</code>) using the same random seed.
        This highlights the speed–quality trade-off in diffusion sampling.
      </p>

      <!-- ===== Rocket Ship Samples ===== -->
      <h3>Rocket Ship — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/rocket_5.png" alt="Rocket ship with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The overall rocket shape is visible, but the scene is very noisy
            and grainy. Background structure is muddled and fine details are poorly defined.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/rocket_20.png" alt="Rocket ship with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> The rocket is crisp with clean edges, smooth gradients, and
            clear light sources. Increasing the number of steps greatly improves coherence and detail.
          </figcaption>
        </figure>
      </div>

      <!-- ===== Snowy Village Samples ===== -->
      <h3>Snowy Mountain Village — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/oil_painting_5.png" alt="Snowy village with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The village is only loosely defined; buildings and trees blur
            into a textured pointillist pattern. The scene feels abstract and lacks clear depth.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/oil_painting_20.png" alt="Snowy village with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> Houses, trees, and snow banks are sharp and stylized, with
            strong color contrasts and readable geometry. Longer sampling recovers a much more
            coherent landscape.
          </figcaption>
        </figure>
      </div>

      <!-- ===== Man Wearing a Hat Samples ===== -->
      <h3>Man Wearing a Hat — 5 vs 20 Sampling Steps</h3>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_a/man_5.png" alt="Man in hat with 5 sampling steps">
          <figcaption>
            <strong>5 steps:</strong> The portrait is heavily textured and stylized, with facial
            features still recognizable but soft and noisy. The image looks more like a rough
            pointillist painting.
          </figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_a/man_20.png" alt="Man in hat with 20 sampling steps">
          <figcaption>
            <strong>20 steps:</strong> The face, hat, and clothing are sharply defined and realistic.
            Noise is largely removed, and lighting and shading look much more natural.
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        Across all three prompts, using only <code>5</code> denoising steps produces quick but noisy
        outputs with blurred edges and unstable structure. Increasing to
        <code>20</code> steps substantially improves global coherence and fine detail, but at the
        cost of additional compute time. For the remainder of the project I use a moderate number of
        sampling steps to balance runtime with visual quality.
      </p>
    </section>


        <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2>Part 1 — Forward &amp; Reverse Processes</h2>
      <p class="summary">
        Part&nbsp;1 focuses on how diffusion models add and remove noise. I first implement the
        forward noising process, then compare traditional Gaussian denoising to a learned UNet,
        and finally run an iterative DDPM-style reverse process starting from a highly corrupted
        image.
      </p>
    </section>

    <!-- ===== Part 1.1 — Forward Process ===== -->
    <section id="part1-forward">
      <h3>1.1 Forward Process (Adding Noise)</h3>
      <p class="summary">
        The forward process corrupts a clean image <code>x₀</code> into a noisy version
        <code>x<sub>t</sub></code> according to
        <code>x<sub>t</sub> = √&alpha;̄<sub>t</sub>x₀ + √(1−&alpha;̄<sub>t</sub>)&epsilon;</code>.
        Below I show the original image and its noised versions at
        <code>t = 250, 500, 750</code> using the provided noise schedule
        <code>alphas_cumprod</code>.
      </p>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original test image">
          <figcaption>Original test image <code>x₀</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code>; content is still very recognizable.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption><code>t = 500</code>; edges soften and textures start to disappear.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption><code>t = 750</code>; the image is close to pure Gaussian noise.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        As <code>t</code> increases, the signal is both rescaled and mixed with noise. At medium
        timesteps the global structure is barely visible, and by <code>t = 750</code> almost all
        semantic information has been destroyed, which motivates the need for a strong reverse
        process.
      </p>
    </section>

    <!-- ===== Part 1.2 — Gaussian Denoising ===== -->
    <section id="part1-gaussian">
      <h3>1.2 Traditional Denoising (Gaussian Blur)</h3>
      <p class="summary">
        I attempt to recover the original image from the noisy versions using Gaussian blur
        (a purely classical, non-learned method). For each timestep I show the noisy input and
        the best Gaussian-denoised result I could obtain by tuning the blur parameters.
      </p>

      <h4>t = 250</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_250.png" alt="Gaussian denoised t=250">
          <figcaption>
            Gaussian denoised at <code>t = 250</code>; some noise is removed, but edges are noticeably blurred.
          </figcaption>
        </figure>
      </div>

      <h4>t = 500</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption>Noisy image at <code>t = 500</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_500.png" alt="Gaussian denoised t=500">
          <figcaption>
            Gaussian denoised at <code>t = 500</code>; the image becomes smoother but also loses most fine detail.
          </figcaption>
        </figure>
      </div>

      <h4>t = 750</h4>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption>Noisy image at <code>t = 750</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_750.png" alt="Gaussian denoised t=750">
          <figcaption>
            Gaussian denoised at <code>t = 750</code>; almost all structure is washed out into a smooth texture.
          </figcaption>
        </figure>
      </div>

      <p class="discussion">
        Gaussian blur can reduce high-frequency noise, but it has no knowledge of natural image
        statistics. At low noise levels it produces passable results, but at higher timesteps it
        simply smooths everything, destroying edges and semantics. This illustrates why diffusion
        models rely on a learned denoiser instead of traditional filtering.
      </p>
    </section>

    <!-- ===== Part 1.3 — One-Step UNet Denoising ===== -->
    <section id="part1-unet">
      <h3>1.3 One-Step Denoising with a Pretrained UNet</h3>
      <p class="summary">
        I next use the pretrained Stage&nbsp;1 UNet (<code>stage_1.unet</code>) to predict the noise
        in <code>x<sub>t</sub></code> and reconstruct an estimate of the clean image via
        <code>ĥx₀ = (x<sub>t</sub> − √(1−&alpha;̄<sub>t</sub>) ĥ&epsilon;)/√&alpha;̄<sub>t</sub></code>.
        For each timestep I show the original, the noisy input, and the one-step denoised result.
      </p>

      <h4>t = 250</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=250">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_250.png" alt="Noisy image t=250">
          <figcaption>Noisy image at <code>t = 250</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_250.png" alt="One-step denoised t=250">
          <figcaption>One-step UNet denoised result at <code>t = 250</code>.</figcaption>
        </figure>
      </div>

      <h4>t = 500</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=500">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_500.png" alt="Noisy image t=500">
          <figcaption>Noisy image at <code>t = 500</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_500.png" alt="One-step denoised t=500">
          <figcaption>One-step UNet denoised result at <code>t = 500</code>.</figcaption>
        </figure>
      </div>

      <h4>t = 750</h4>
      <div class="grid three">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image t=750">
          <figcaption>Original image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_750.png" alt="Noisy image t=750">
          <figcaption>Noisy image at <code>t = 750</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_750.png" alt="One-step denoised t=750">
          <figcaption>One-step UNet denoised result at <code>t = 750</code>.</figcaption>
        </figure>
      </div>

      <p class="discussion">
        At moderate noise levels (e.g., <code>t = 250</code>), a single UNet pass produces a
        surprisingly accurate reconstruction. As the noise level increases, the one-step estimate
        still produces a plausible natural image but drifts further away from the exact original,
        showing that multiple steps are preferable when starting from heavily corrupted inputs.
      </p>
    </section>

    <!-- ===== Part 1.4 — Iterative Denoising ===== -->
    <section id="part1-iter">
      <h3>1.4 Iterative Denoising with Strided Timesteps</h3>
      <p class="summary">
        Finally, I implement an iterative reverse diffusion process using a strided schedule of
        timesteps. Starting from a very noisy image at <code>t = 690</code>, the model repeatedly
        predicts noise and steps toward <code>t = 0</code>, applying the DDPM update rule with
        learned variance. Below are snapshots of the image every few iterations.
      </p>

      <div class="grid six">
        <figure class="card">
          <img src="./media/part_b/iter_0.png" alt="Iterative denoise step 0">
          <figcaption>Iteration 0 (starting from <code>t = 690</code>): extremely noisy.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_5.png" alt="Iterative denoise step 5">
          <figcaption>Iteration 5: coarse shapes begin to emerge.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_10.png" alt="Iterative denoise step 10">
          <figcaption>Iteration 10: major structures are visible.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_15.png" alt="Iterative denoise step 15">
          <figcaption>Iteration 15: edges and textures sharpen.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_20.png" alt="Iterative denoise step 20">
          <figcaption>Iteration 20: image is close to a clean sample.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iter_22.png" alt="Iterative denoise final">
          <figcaption>Iteration 22: final iterative denoised image.</figcaption>
        </figure>
      </div>

      <h4>Final Comparison at t = 690</h4>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part_b/original.png" alt="Original image">
          <figcaption>Original clean image.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/t_690.png" alt="Noisy image t=690">
          <figcaption>Noisy image at <code>t = 690</code> (starting point).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/iterative_690.png" alt="Iterative denoise result">
          <figcaption>Iterative denoising result (multiple DDPM steps).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/one_step_690.png" alt="One-step denoise result">
          <figcaption>One-step UNet denoise from <code>t = 690</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part_b/guassian_690.png" alt="Gaussian blur result">
          <figcaption>Gaussian blur baseline from <code>t = 690</code>.</figcaption>
        </figure>
      </div>

      <p class="discussion">
        The sequence of iterated images shows the diffusion model gradually pulling the sample back
        onto the natural image manifold. Compared to the one-step UNet and Gaussian blur, iterative
        denoising produces the cleanest and most coherent result, especially when starting from a
        very noisy state. The Gaussian version is overly smooth and loses structure, while the
        one-step UNet tends to under-correct large amounts of noise.
      </p>
    </section>


    <!-- ===================== Part 2 ===================== -->
    <section id="part2">
      <h2>Part 2 — Sampling, Classifier-Free Guidance, and SDEdit</h2>
      <p class="summary">
        Part&nbsp;2 uses the iterative sampler to generate images from pure noise, adds classifier-free
        guidance (CFG) to improve visual quality, and then applies the model to image-to-image
        translation via SDEdit, including edits on my own photographs.
      </p>
    </section>

    <!-- ===== Part 2.1 — Sampling from Noise ===== -->
    <section id="part2-sampling">
      <h3>2.1 Sampling from Random Noise</h3>
      <p class="summary">
        Starting from random Gaussian noise at the Stage&nbsp;1 resolution, I run the
        <code>iterative_denoise</code> pipeline with <em>"a high quality photo"</em> as the text
        condition. This produces unconditional samples from the learned image distribution.
      </p>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part2/sampling_sample1.png" alt="Sample 1 from noise">
          <figcaption>Sample 1 generated from pure noise.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sampling_sample2.png" alt="Sample 2 from noise">
          <figcaption>Sample 2 generated from pure noise.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sampling_sample3.png" alt="Sample 3 from noise">
          <figcaption>Sample 3 generated from pure noise.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sampling_sample4.png" alt="Sample 4 from noise">
          <figcaption>Sample 4 generated from pure noise.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sampling_sample5.png" alt="Sample 5 from noise">
          <figcaption>Sample 5 generated from pure noise.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        Without guidance, the samples are diverse but sometimes lack crisp structure or clear focal
        points. This is expected: the model is encouraged to match the overall data distribution, not
        any specific concept. In the next section, classifier-free guidance trades some diversity for
        more visually striking images.
      </p>
    </section>

    <!-- ===== Part 2.2 — Classifier-Free Guidance ===== -->
    <section id="part2-cfg">
      <h3>2.2 Classifier-Free Guidance (CFG)</h3>
      <p class="summary">
        I implement <code>iterative_denoise_cfg</code>, which combines conditional and unconditional
        noise estimates <code>&epsilon<sub>c</sub></code> and <code>&epsilon<sub>u</sub></code> as
        <code>&epsilon = &epsilon<sub>u</sub> + γ(&epsilon<sub>c</sub> − &epsilon<sub>u</sub>)</code>.
        I then sample images for <em>"a high quality photo"</em> with guidance scale
        <code>γ = 7</code>.
      </p>
      <div class="grid five">
        <figure class="card">
          <img src="./media/part2/cfg_sample1.png" alt="CFG sample 1">
          <figcaption>CFG sample 1 with <code>γ = 7</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/cfg_sample2.png" alt="CFG sample 2">
          <figcaption>CFG sample 2 with <code>γ = 7</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/cfg_sample3.png" alt="CFG sample 3">
          <figcaption>CFG sample 3 with <code>γ = 7</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/cfg_sample4.png" alt="CFG sample 4">
          <figcaption>CFG sample 4 with <code>γ = 7</code>.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/cfg_sample5.png" alt="CFG sample 5">
          <figcaption>CFG sample 5 with <code>γ = 7</code>.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        Compared to unguided sampling, CFG significantly sharpens edges and encourages stronger,
        more coherent global structure (for example, clearer subjects and backgrounds). However,
        high guidance values can sometimes over-constrain the model and reduce diversity or introduce
        saturation artifacts. I found <code>γ = 7</code> to be a good trade-off for this project.
      </p>
    </section>

    <!-- ===== Part 2.3 — Image-to-Image Translation (SDEdit) ===== -->
    <section id="part2-sdedit">
      <h3>2.3 Image-to-Image Translation with SDEdit</h3>
      <p class="summary">
        In this section I apply SDEdit: I add noise to existing images (the provided test image and
        two of my own photos) and then denoise them with <code>iterative_denoise_cfg</code>. By
        changing how many denoising steps we run (or equivalently, where we start in the schedule),
        we control how large the “edit” is: small edits preserve structure closely, while larger edits
        push the image toward a new but related sample.
      </p>
      <h4>Test Image Edits</h4>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part2/sdedit_test_original.png" alt="Original test image for SDEdit">
          <figcaption>Original test image (no noise).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_test_i1.png" alt="Test image edit i_start=1">
          <figcaption>Test image edit with <code>i_start = 1</code> (very light edit).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_test_i7.png" alt="Test image edit i_start=7">
          <figcaption><code>i_start = 7</code>; more creative drift while preserving composition.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_test_i20.png" alt="Test image edit i_start=20">
          <figcaption><code>i_start = 20</code>; large edits, image resembles a new sample.</figcaption>
        </figure>
      </div>

      <h4>Edits on My Own Photographs</h4>
      <div class="grid four">
        <figure class="card">
          <img src="./media/part2/sdedit_dc_original.png" alt="My DC photo original">
          <figcaption>My own DC image (original).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_dc_i10.png" alt="My DC photo SDEdit result">
          <figcaption>DC image SDEdit result (<code>i_start = 10</code>), then upsampled to 256&times;256.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_fuji_original.png" alt="My Fuji photo original">
          <figcaption>My own Mount Fuji image (original).</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part2/sdedit_fuji_i10.png" alt="My Fuji photo SDEdit result">
          <figcaption>Fuji image SDEdit result (<code>i_start = 10</code>), then upsampled to 256&times;256.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        With small noise levels, the SDEdit outputs look like lightly stylized versions of the
        originals: lighting, textures, and minor details change while the global layout stays the
        same. As I increase the starting noise level, the model becomes more “creative” and the
        outputs eventually resemble new scenes that merely echo the composition of the inputs.
        On my own photos, this gives a neat way to transform real-world shots into painterly,
        model-generated interpretations.
      </p>
    </section>

    <!-- ===================== Part 3 ===================== -->
    <section id="part3">
      <h2>Part 3 — Visual Anagrams</h2>
      <p class="summary">
        For the graduate-level component, I implement visual anagrams based on the CVPR&nbsp;2024
        paper. The goal is to generate a single image that looks like one prompt when viewed
        upright and another when flipped upside down, by combining noise estimates from two
        differently conditioned UNet passes.
      </p>
    </section>

    <!-- ===== Part 3 — Old Man / Campfire Illusion ===== -->
    <section id="part3-anagram-main">
      <h3>Visual Anagram: Old Man &amp; Campfire</h3>
      <p class="summary">
        I construct an image that resembles <em>"an oil painting of an old man"</em> when upright
        and <em>"an oil painting of people around a campfire"</em> when flipped. At each diffusion
        step, I:
      </p>
      <ul>
        <li>Denoise <code>x<sub>t</sub></code> with prompt <code>p₁</code> (old man) to get <code>&epsilon₁</code>.</li>
        <li>Flip <code>x<sub>t</sub></code>, denoise with prompt <code>p₂</code> (campfire), and flip the output back to get <code>&epsilon₂</code>.</li>
        <li>Average the noise estimates <code>&epsilon = (&epsilon₁ + &epsilon₂)/2</code> and perform one reverse diffusion step.</li>
      </ul>
      <div class="grid two">
        <figure class="card">
          <img src="./media/part3/anagram_oldman_upright_256.png" alt="Upright">
          <figcaption>Upright image of an old man.</figcaption>
        </figure>
        <figure class="card">
          <img src="./media/part3/anagram_oldman_flipped_256.png" alt="Flipped">
          <figcaption>Flipped image of a man behind a fire.</figcaption>
        </figure>
      </div>
      <p class="discussion">
        The illusion works because local features can be interpreted in two different ways depending
        on orientation. By averaging noise estimates conditioned on two prompts, the sampler finds
        a compromise that is consistent with both descriptions. Getting a strong illusion requires
        some trial-and-error: different random seeds and guidance strengths can produce images that
        favor one prompt more than the other.
      </p>
    </section>


  </main>

  <footer class="site-footer">
    <p>© <span id="year"></span> Compton Ross</p>
  </footer>

  <!-- Zoom modal -->
  <div id="imgModal" class="modal">
    <span id="modalClose" class="close">&times;</span>
    <img class="modal-content" id="modalImg" alt="">
    <div id="caption"></div>
  </div>

  <script>
    // Click-to-zoom behavior
    const modal = document.getElementById("imgModal");
    const modalImg = document.getElementById("modalImg");
    const captionText = document.getElementById("caption");
    const closeBtn = document.getElementById("modalClose");

    document.querySelectorAll("section img").forEach(img => {
      img.style.cursor = "zoom-in";
      img.addEventListener("click", () => {
        modal.style.display = "block";
        modalImg.src = img.src;
        captionText.textContent =
          img.nextElementSibling ? img.nextElementSibling.innerText : "";
      });
    });

    closeBtn.addEventListener("click", () => {
      modal.style.display = "none";
    });

    modal.addEventListener("click", (e) => {
      if (e.target === modal) modal.style.display = "none";
    });

    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
